{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How-to for the Sliding Window Algorithm\n",
    "\n",
    "This notebook shows how to load a pre-trained model and run the sliding window algorithm to run object detection on a complex scene containing several objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import glob, os\n",
    "\n",
    "import data.data_helpers as data_helpers\n",
    "import models.model_helpers as model_helpers\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data\n",
    "\n",
    " - First, we load the base dataframe which points to all images in the __validation set__. We have to use the validation set here, to get an unbiased overall score of end-to-end object recognition pipeline.\n",
    " \n",
    "  - For validation, we use a two-fold approach: \n",
    "     - Validation on an artifically generated dataset with randomized objects from the validation set.\n",
    "     - Validation on completely unseend images, realistic fridge scenes which were manually selected before. \n",
    " \n",
    " \n",
    " - We could theoretically, use base dataframe, to generate a new artificial dataset with randomize object positions (this is shown in the commented block). Alternatively, artifical datasets can be created with the script *data/generate_artifical_validation_set.py*\n",
    " \n",
    " - Instead of generating a new artificial dataset here, we just load one which is already contained in the folder *data/validation_artificial*.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base data frame\n",
    "\n",
    "data_directories = [\n",
    "    'data/FIDS30',\n",
    "    'data/original_clean',\n",
    "    'data/update_9sep'\n",
    "]\n",
    "\n",
    "test_size = 0.1\n",
    "seed = 11\n",
    "\n",
    "# get base data frame and all class names\n",
    "_, data_df_test, classes, class2ind, ind2class = \\\n",
    "data_helpers.get_train_test_data_df(data_directories, test_size, seed)\n",
    "\n",
    "# get validation set with \"real\" fridge scenes\n",
    "val_real_data = data_helpers.get_validation_dict('data/validation_real/', classes, verbose=0)\n",
    "\n",
    "# get validation set with \"artificial\" fridge scenes\n",
    "val_artificial_data = data_helpers.get_validation_dict('data/validation_artificial/', classes, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GENERATE NEW ARTIFICIAL VALIDATION SET\n",
    "\n",
    "#N_samples = 10\n",
    "#N_min = 5\n",
    "#N_max = 10\n",
    "#spacing = 150\n",
    "#size_jitter=(0.9,1.5)\n",
    "#seed = 11\n",
    "#bg_path = 'data/artificial_background'\n",
    "\n",
    "#val_artificial_data = data_helpers.generate_artifical_validation_dataset(data_df_test, bg_path, N_samples, N_min, N_max, spacing, size_jitter, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show examples pictures\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "ax = plt.subplot(121)\n",
    "plt.imshow(val_real_data[15]['image'])\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.text(0.05, 0.98, '\\n'.join(val_real_data[15]['labels']), \n",
    "         transform = ax.transAxes, verticalalignment='top',\n",
    "         bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "plt.imshow(val_artificial_data[15]['image'])\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.text(0.05, 0.98, '\\n'.join(val_artificial_data[15]['labels']), \n",
    "         transform = ax.transAxes, verticalalignment='top',\n",
    "         bbox=dict(facecolor='red', alpha=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, save_model, load_model\n",
    "\n",
    "loaded_model = load_model(\n",
    "    \"models/test_model2\",\n",
    "    custom_objects=None,\n",
    "    compile=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect objects using \"image pyramid\"\n",
    "\n",
    "    - First, generate an \"image pyramid\", i.e. scale the given image by different factors to enable object detection of \n",
    "    differently sized objects. \n",
    "    \n",
    "    - Here, for simplicity, we only use one scaling factor. (So this is not really an image pyramid)\n",
    "    \n",
    "    - Run non-maximum suppression on predicted boxes.\n",
    "    \n",
    "For more info on the \"image pyramid\" see, e.g.:\n",
    "https://www.pyimagesearch.com/2018/05/14/a-gentle-guide-to-deep-learning-object-detection/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_resnet50\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as preprocess_mobilenet_v2\n",
    "\n",
    "# define correct pre-processing function for the network\n",
    "preprocess_func = lambda x: x/255.\n",
    "\n",
    "# define correct input size for the network (the one it was trained on)\n",
    "kernel_size = 224\n",
    "\n",
    "# define selection threshold / do not take prediction with a lesser confidence level\n",
    "thr = 0.9\n",
    "\n",
    "# define non-max suppression threshold\n",
    "overlap_thr = 0.1\n",
    "\n",
    "# define image pyramid (object sizes / larger factors correspond to smaller objects)\n",
    "scaling_factors = [1.0, 2.0]\n",
    "sliding_strides = [64, 128]\n",
    "\n",
    "# select image to perform predictions on\n",
    "#input_sample = val_artificial_data[15]\n",
    "input_sample = val_real_data[15]\n",
    "img = input_sample['image']\n",
    "\n",
    "# perform object detection with final model\n",
    "pred_labels, probabilities, x0, y0, windowsize = \\\n",
    "        model_helpers.object_detection_sliding_window(model=loaded_model, \n",
    "                                                      input_img=img, \n",
    "                                                      preprocess_function=preprocess_func, \n",
    "                                                      kernel_size=kernel_size, \n",
    "                                                      ind2class=ind2class, \n",
    "                                                      scaling_factors=scaling_factors, \n",
    "                                                      sliding_strides=sliding_strides, \n",
    "                                                      thr=thr, \n",
    "                                                      overlap_thr=overlap_thr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_labels = input_sample['labels']\n",
    "\n",
    "# true positives are intersection between predicted and actual labels\n",
    "TP = set(actual_labels).intersection(set(pred_labels))\n",
    "\n",
    "# false positives are difference between predicted and actual labels\n",
    "FP = set(pred_labels) - set(actual_labels)\n",
    "\n",
    "# false negatives are difference between actual and predicted labels\n",
    "FN = set(actual_labels) - set(pred_labels)\n",
    "\n",
    "# true negatives are intersection between the differences between all classes \n",
    "# and the actual and predicted classes respectively\n",
    "# (usually not so important for object detection)\n",
    "TN = (set(classes) - set(actual_labels)).intersection((set(classes) - set(pred_labels)))\n",
    "\n",
    "precision = len(TP) / (len(TP)+len(FP))\n",
    "recall  = len(TP) / (len(TP)+len(FN))\n",
    "\n",
    "print(f'Correctly identified: ' + ', '.join(TP))\n",
    "print(f'Missed items: ' + ', '.join(FN))\n",
    "print()\n",
    "print(f'Precision: {100*precision:.2f} %')\n",
    "print(f'Recall: {100*recall:.2f} %')\n",
    "\n",
    "# visualize results\n",
    "fig = model_helpers.visualize_predictions(img, \n",
    "                                          pred_labels, \n",
    "                                          probabilities, \n",
    "                                          x0, \n",
    "                                          y0,\n",
    "                                          windowsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_food",
   "language": "python",
   "name": "deep_food"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
