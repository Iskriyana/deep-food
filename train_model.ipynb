{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How-to train an object detection model\n",
    "\n",
    "This notebook shows how run one \"experiment\" and train a new model. \n",
    "\n",
    "Training consists of two parts:\n",
    "   1. Training a CNN for multi-classification on **single** food items (using transfer learning).\n",
    "   2. Tuning the sliding window algorithm for optimal performance on both valdiation sets (real and artificial).\n",
    "    \n",
    "This notebook carries you through the process and explains the individual steps.\n",
    "\n",
    "An automatized script which goes through the whole process is given in **XXXX**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, os\n",
    "import datetime\n",
    "\n",
    "import data.data_helpers as data_helpers\n",
    "import models.model_helpers as model_helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parameter dictionary\n",
    "\n",
    "This dictionary will be saved along with the trained model to log the results. \n",
    "\n",
    "Things to do before running a new experiment:\n",
    "   1. Define folder from which to take data\n",
    "   1. Define experiment name\n",
    "   3. Define model architecture and other hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whether to log the results to json:\n",
    "logging = True\n",
    "\n",
    "# whether to save the model:\n",
    "saving  = True\n",
    "\n",
    "### Specify folders from which to take data:\n",
    "data_directories = [\n",
    "    'data/FIDS30',\n",
    "    'data/original_clean',\n",
    "    'data/update_9sep'\n",
    "]\n",
    "\n",
    "real_validation_path = 'data/validation_real/'\n",
    "real_artificial_path = 'data/validation_artificial/'\n",
    "\n",
    "\n",
    "PARAMS = {\n",
    "\n",
    "    ### Define experiment name:\n",
    "    'experiment_name': 'test_all',\n",
    "    \n",
    "    ### Parameters for the CNN:\n",
    "    'data_directories': data_directories,\n",
    "    'test_size': 0.1,\n",
    "    'seed': 11,\n",
    "    'batch_size': 32,\n",
    "    'target_size': (112,112),\n",
    "    'epochs_cold': 1,\n",
    "    'epochs_finetune': 1,\n",
    "    'lr_cold': 0.001,\n",
    "    'lr_finetune': 1e-5,\n",
    "    \n",
    "    'base_net': 'mobilenet_v2', # supported: resnet50/mobilenet_v2\n",
    "    'head_net': '[tf.keras.layers.GlobalAveragePooling2D(),\\\n",
    "                 tf.keras.layers.Dropout(0.2)]',\n",
    "    \n",
    "    # The following parameters are for tuning the sliding window algorithm:\n",
    "    'real_validation_path': real_validation_path,\n",
    "    'artificial_validation_path': real_artificial_path,\n",
    "    'thr_list': [0.9, 0.93, 0.96],\n",
    "    'overlap_thr_list': [0.2, 0.3, 0.5],#list(np.arange(0,1,0.05)),\n",
    "    'scaling_factors': [1.0, 1.5, 2.0],\n",
    "    'sliding_strides': [32, 64, 128]\n",
    "}\n",
    "\n",
    "logdir = f'logs/experiments/{PARAMS[\"experiment_name\"]}_' + datetime.datetime.now().strftime(\"%Y_%m_%d-%H:%M\")\n",
    "logdir_tb = f'logs/scalars/{PARAMS[\"experiment_name\"]}_' + datetime.datetime.now().strftime(\"%Y_%m_%d-%H:%M\")\n",
    "\n",
    "# create logging folder and tensorboard callback function\n",
    "if logging:\n",
    "    print(f'Log results to {logdir}')\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "        \n",
    "    tensorboard_callbacks = [tf.keras.callbacks.TensorBoard(log_dir=logdir_tb)]\n",
    "else:\n",
    "    logdir = ''\n",
    "    logdir_tb = ''\n",
    "    tensorboard_callbacks = []\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_train, data_df_test, classes, class2ind, ind2class = \\\n",
    "data_helpers.get_train_test_data_df(PARAMS['data_directories'], PARAMS['test_size'], PARAMS['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up data pipeline\n",
    "\n",
    " - Use keras-ImageDataGenerator for pre-processing and define data augmentations\n",
    " - Create keras-DataFrameIterators as an input to the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as preprocess_mobilenet_v2\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_resnet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define preprocessing function\n",
    "if PARAMS['base_net'] == 'resnet50':\n",
    "    preprocess_function = preprocess_resnet50\n",
    "elif PARAMS['base_net'] == 'mobilenet_v2':\n",
    "    preprocess_function = preprocess_mobilenet_v2\n",
    "\n",
    "# crea ImageDataGenerator with data augmentations\n",
    "image_generator_train = ImageDataGenerator(horizontal_flip=True,\n",
    "                                           vertical_flip=True,\n",
    "                                           rotation_range=180,\n",
    "                                           shear_range=30,\n",
    "                                           zoom_range=[0.5,1],\n",
    "                                           preprocessing_function=preprocess_function)\n",
    "\n",
    "image_generator_test = ImageDataGenerator(preprocessing_function=preprocess_function)\n",
    "\n",
    "# create data iterators\n",
    "train_iterator = data_helpers.get_data_frame_iterator(data_df_train, image_generator_train, PARAMS)\n",
    "test_iterator = data_helpers.get_data_frame_iterator(data_df_test, image_generator_test, PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show some example pictures from the train set\n",
    "\n",
    " - Use `deprocess_func=None` for MobileNet\n",
    " - Use `deprocess_func=data_helpers.deprocess_imagenet` for Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate example batch\n",
    "image_batch, label_batch = next(train_iterator)\n",
    "\n",
    "# get correct image deprocessing function for proper visualization of images\n",
    "if PARAMS['base_net'] == 'mobilenet_v2':\n",
    "    plot_deprocess_func = None\n",
    "else:\n",
    "    plot_deprocess_func = data_helpers.deprocess_imagenet\n",
    "\n",
    "# generat figure\n",
    "fig = data_helpers.image_grid(image_batch, label_batch, ind2class, n_row=4, n_col=8, deprocess_func=plot_deprocess_func)\n",
    "\n",
    "if logging:\n",
    "    if not os.path.exists(os.path.join(logdir, 'figures')):\n",
    "        os.makedirs(os.path.join(logdir, 'figures'))\n",
    "    fig.savefig(os.path.join(logdir, 'figures/training_examples.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style('darkgrid'):\n",
    "    fig = data_helpers.show_label_distribution(data_df_train, ind2class, 'Train')\n",
    "    if logging:\n",
    "        fig.savefig(os.path.join(logdir, 'figures/training_distribution.png'), bbox_inches='tight')\n",
    "\n",
    "    fig = data_helpers.show_label_distribution(data_df_test, ind2class, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate model\n",
    "\n",
    "Create the model using a transfer learning approach.\n",
    "\n",
    " - Download pre-trained base model\n",
    " - Freeze all layers of the base model \n",
    " - Append a custom head using the Sequential API\n",
    " - Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# download pre-trained model\n",
    "if PARAMS['base_net'] == 'resnet50':\n",
    "    pretrained_model = tf.keras.applications.ResNet50(input_shape=PARAMS['target_size']+(3,), \n",
    "                                                      include_top=False)\n",
    "elif PARAMS['base_net'] == 'mobilenet_v2':\n",
    "    pretrained_model = tf.keras.applications.MobileNetV2(input_shape=PARAMS['target_size']+(3,), \n",
    "                                                      include_top=False)\n",
    "\n",
    "# freeze pre-trained model \n",
    "pretrained_model.trainable = False\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    pretrained_model,\n",
    "    *eval(PARAMS['head_net']),\n",
    "    tf.keras.layers.Dense(len(classes), activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=PARAMS['lr_cold']),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the head of the model (with the base frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = len(train_iterator.filenames)//PARAMS['batch_size']+1\n",
    "\n",
    "history_cold = model.fit_generator(\n",
    "    train_iterator, \n",
    "    validation_data=test_iterator,\n",
    "\tepochs=PARAMS['epochs_cold'], \n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    callbacks=tensorboard_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the head of the model (with the base un-frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model.trainable = True\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=PARAMS['lr_finetune']),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_finetune = model.fit_generator(\n",
    "    train_iterator, \n",
    "    validation_data=test_iterator,\n",
    "\tepochs=PARAMS['epochs_cold']+PARAMS['epochs_finetune'], \n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    callbacks=tensorboard_callbacks,\n",
    "    initial_epoch=PARAMS['epochs_cold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append both histories \n",
    "history = {k: v + history_finetune.history[k] for (k,v) in history_cold.history.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "ax = plt.subplot(121)\n",
    "plt.plot(history['loss'], 'b')\n",
    "plt.plot(history['val_loss'], 'r')\n",
    "plt.ylim([0, None])\n",
    "ax.set_ylabel('loss')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.legend(['train', 'validation'])\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "plt.plot(history['accuracy'], 'b')\n",
    "plt.plot(history['val_accuracy'], 'r')\n",
    "plt.ylim([0,1])\n",
    "ax.set_ylabel('accuracy')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.legend(['train', 'validation'])\n",
    "\n",
    "if logging:\n",
    "    fig.savefig(os.path.join(logdir, 'figures/loss_accuracy_training.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at results\n",
    "\n",
    " - Generate predictions on the whole test set\n",
    " - Look at some predictions\n",
    " - Look at the most wrong predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the whole set\n",
    "images, labels, predict_i, predict_proba, predict_labels, ind_misclassified = model_helpers.predict_on_whole_dataset(model, test_iterator, ind2class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show first 32 images in test set\n",
    "fig = data_helpers.image_grid(images[:32], labels[:32], ind2class, n_row=4, n_col=8, deprocess_func=plot_deprocess_func,\n",
    "                              predict=predict_i[:32], predict_proba=predict_proba[:32], hspace=0.5)\n",
    "\n",
    "if logging:\n",
    "    fig.savefig(os.path.join(logdir, 'figures/test_examples.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show worst misclassifications\n",
    "ind_sorted = np.argsort(predict_proba[ind_misclassified])[::-1]\n",
    "ind_sorted = ind_misclassified[ind_sorted]\n",
    "\n",
    "fig = data_helpers.image_grid(images[ind_sorted], labels[ind_sorted], ind2class, n_row=4, n_col=8, deprocess_func=plot_deprocess_func,\n",
    "                              predict=predict_i[ind_sorted], predict_proba=predict_proba[ind_sorted], hspace=0.5)\n",
    "\n",
    "if logging:\n",
    "    fig.savefig(os.path.join(logdir, 'figures/worst_test_examples.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get confusion matrix and metrics \n",
    "\n",
    " - Get model performance metrics\n",
    " - Get confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate evaluation dataframe\n",
    "eval_df = pd.DataFrame({'predicted': predict_i, 'actual': np.nonzero(labels)[1]})\n",
    "eval_df.predicted = eval_df.predicted.apply(lambda x: ind2class[x])\n",
    "eval_df.actual = eval_df.actual.apply(lambda x: ind2class[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "accuracy = accuracy_score(eval_df.predicted, eval_df.actual)\n",
    "micro_precision = precision_score(eval_df.predicted, eval_df.actual, average='micro')\n",
    "micro_recall = recall_score(eval_df.predicted, eval_df.actual, average='micro')\n",
    "macro_precision = precision_score(eval_df.predicted, eval_df.actual, average='macro')\n",
    "macro_recall = recall_score(eval_df.predicted, eval_df.actual, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\\nMicro-precision: {micro_precision}\\nMacro-precision: {macro_precision}\\nMicro-recall: {micro_recall}\\nMacro-recall: {macro_recall}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix = pd.DataFrame(confusion_matrix(eval_df['actual'], eval_df['predicted'], labels=classes, normalize='true'),\n",
    "                           index=classes, columns=classes)\n",
    "fig, ax = plt.subplots(figsize=(20,15))\n",
    "sns.heatmap((conf_matrix*100).astype(np.int), annot=True)\n",
    "\n",
    "if logging:\n",
    "    fig.savefig(os.path.join(logdir, 'figures/confusion_matrix.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log results and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import save_model\n",
    "import json\n",
    "\n",
    "# define results dictionary\n",
    "results = {'params': PARAMS,\n",
    "           'eval_df': eval_df.to_dict(), \n",
    "           'history': history, \n",
    "           'metrics': {'accuracy': accuracy, \n",
    "                       'micro-precision': micro_precision, \n",
    "                       'macro-precision': macro_precision, \n",
    "                       'micro-recall': micro_recall, \n",
    "                       'macro-recall': macro_recall, \n",
    "           'model_path': logdir}}\n",
    "\n",
    "# save the model\n",
    "if saving:\n",
    "    save_model(model, logdir)\n",
    "\n",
    "# save logging dict to json-file\n",
    "if logging:\n",
    "    with open(os.path.join(logdir, 'results_classifier.json'), 'w+') as f:\n",
    "        json.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune the sliding window algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get validation set with \"real\" fridge scenes\n",
    "val_real_data = data_helpers.get_validation_dict(PARAMS['real_validation_path'], classes, verbose=0)\n",
    "\n",
    "# get validation set with \"artificial\" fridge scenes\n",
    "val_artificial_data = data_helpers.get_validation_dict(PARAMS['artificial_validation_path'], classes, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show examples pictures from validation set\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "ax = plt.subplot(121)\n",
    "plt.imshow(val_real_data[15]['image'])\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.text(0.05, 0.98, '\\n'.join(val_real_data[15]['labels']), \n",
    "         transform = ax.transAxes, verticalalignment='top',\n",
    "         bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "plt.imshow(val_artificial_data[15]['image'])\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.text(0.05, 0.98, '\\n'.join(val_artificial_data[15]['labels']), \n",
    "         transform = ax.transAxes, verticalalignment='top',\n",
    "         bbox=dict(facecolor='red', alpha=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run sliding window algorithm on validation set\n",
    "\n",
    "The following cell runs the sliding window algorithm with the specified parameters over the whole validation set (both artificial and real) and saves intermediate results in a temporaray dataframe `sliding_df`.\n",
    "\n",
    "The algorithm does not yet apply thresholding or non-maximum suppression. This dataframe therefore contains the classification results for __ALL__ boxes (without thresholding). \n",
    "\n",
    " - `scaling_factors` are the different scaling factors for the image pyramid\n",
    " - `sliding_strides` the are different strides for each level of the pyramid\n",
    " \n",
    "Using the results from this dataframe `sliding_df`, we can later perform thresholding and non-maximum suppression (which require a lot less computational power than the image classification itself) and find their optimal values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.tuning_helpers as tuning_helpers\n",
    "\n",
    "sliding_df = tuning_helpers.tuning_loop_sliding_window(scaling_factors=PARAMS['scaling_factors'], \n",
    "                                        sliding_strides=PARAMS['sliding_strides'],\n",
    "                                        val_real_data=val_real_data, \n",
    "                                        val_artificial_data=val_artificial_data, \n",
    "                                        ind2class=ind2class,\n",
    "                                        model=model, \n",
    "                                        preprocess_func=preprocess_function, \n",
    "                                        kernel_size=PARAMS['target_size'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import tqdm\n",
    "\n",
    "# get all combinations of pyramid elements as list of index tuples\n",
    "N_pyramid = len(PARAMS['scaling_factors'])\n",
    "pyramid_combs = []\n",
    "for n in range(1, N_pyramid+1):\n",
    "    pyramid_combs.extend(combinations(range(N_pyramid), n))\n",
    "\n",
    "\n",
    "# Iterate over all samples in both datasets, test all combinations of hyperparemeters\n",
    "# and evaluate the final performance of the object detection algorithm.\n",
    "tuning_df = []\n",
    "\n",
    "for sample in tqdm.tqdm(sliding_df):\n",
    "    data_type = sample['data_type']\n",
    "    # iterate over all values for the decision treshold\n",
    "    for thr in PARAMS['thr_list']:\n",
    "        # iterate over all values for the non-max suppresion threshold\n",
    "        for overlap_thr in PARAMS['overlap_thr_list']:\n",
    "            # iterate over all combination of pyramid levels / object sizes\n",
    "            for comb_ind in pyramid_combs:\n",
    "\n",
    "\n",
    "                pred_labels, probabilities, x0, y0, windowsize = \\\n",
    "                    model_helpers.combine_pyramid_predictions(comb_ind,\n",
    "                                                              sample['pyramid_pred_labels'],\n",
    "                                                              sample['pyramid_probabilities'],\n",
    "                                                              sample['pyramid_x0'],\n",
    "                                                              sample['pyramid_y0'],\n",
    "                                                              sample['pyramid_windowsize'])\n",
    "\n",
    "                # apply decision threshold\n",
    "                mask = np.array(probabilities)>thr\n",
    "                pred_labels = pred_labels[mask]\n",
    "                probabilities = probabilities[mask]\n",
    "                x0 = x0[mask]\n",
    "                y0 = y0[mask]\n",
    "                windowsize = windowsize[mask]\n",
    "\n",
    "                # apply non-maximum suppression algorithm\n",
    "                pred_labels, probabilities, x0, y0, windowsize = model_helpers.nonmax_suppression(pred_labels, \n",
    "                                                                                      probabilities, \n",
    "                                                                                      x0, \n",
    "                                                                                      y0, \n",
    "                                                                                      windowsize, \n",
    "                                                                                      overlap_thr=overlap_thr)\n",
    "\n",
    "                # get evaluation metrics\n",
    "                actual_labels = sample['actual_labels']\n",
    "                accuracy, precision, recall, TP, FP, TN, FN = tuning_helpers.get_evaluation_metrics(actual_labels, pred_labels, classes)\n",
    "\n",
    "                # log results\n",
    "                scaling_factors = np.array(PARAMS['scaling_factors'])[list(comb_ind)]\n",
    "                sliding_strides = np.array(PARAMS['sliding_strides'])[list(comb_ind)]\n",
    "\n",
    "                log = {'data_type': sample['data_type'],\n",
    "                       'i_img': sample['i_img'], \n",
    "                       'thr': thr,\n",
    "                       'overlap_thr': overlap_thr,\n",
    "                       'scaling_factors': scaling_factors.tolist(),\n",
    "                       'sliding_strides': sliding_strides.tolist(),\n",
    "\n",
    "                       'accuracy': accuracy, \n",
    "                       'precision': precision, \n",
    "                       'recall': recall, \n",
    "                       'TP': list(TP), \n",
    "                       'FP': list(FP), \n",
    "                       'TN': list(TN), \n",
    "                       'FN': list(FN), \n",
    "                       #'actual_labels': list(actual_labels), \n",
    "                       #'predicted_labels': pred_labels.tolist(),\n",
    "                       #'probabilities': probabilities.tolist(), \n",
    "                       #'x0': x0.tolist(), \n",
    "                       #'y0': y0.tolist(), \n",
    "                       #'windowsize': windowsize.tolist()\n",
    "                       }\n",
    "\n",
    "                tuning_df.append(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tuning results to json-file\n",
    "if logging:\n",
    "    with open(os.path.join(logdir, 'results_tuning.json'), 'w+') as f:\n",
    "        json.dump(tuning_df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select combination of hyperparameters with highest F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get summary metrics for each set of sliding window parameters\n",
    "metrics_df = pd.DataFrame(tuning_df)\n",
    "metrics_df.scaling_factors = metrics_df.scaling_factors.astype(str)\n",
    "\n",
    "metrics_df = metrics_df.groupby(['data_type', 'thr', 'overlap_thr', 'scaling_factors'])['precision', 'recall'].mean(0)\n",
    "metrics_df['f1'] = 2*metrics_df.precision*metrics_df.recall/(metrics_df.precision + metrics_df.recall)\n",
    "metrics_df = metrics_df.reset_index()\n",
    "\n",
    "# save summary metrics evaluation to json\n",
    "if logging:\n",
    "    metrics_df.to_json(os.path.join(logdir, 'metrics_df.json'))\n",
    "\n",
    "# aggregate metrics per type of dataset\n",
    "metrics_per_dataset = metrics_df.pivot(index=['thr', 'overlap_thr', 'scaling_factors'], \n",
    "                                       columns=['data_type'], \n",
    "                                       values=['f1', 'precision', 'recall'])\n",
    "new_columns = [a + '_' + b for (a,b) in metrics_per_dataset.columns]\n",
    "metrics_per_dataset.columns = new_columns\n",
    "metrics_per_dataset['f1'] = (metrics_per_dataset['f1_artificial'] +  metrics_per_dataset['f1_real'])/2.\n",
    "metrics_per_dataset['precision'] = (metrics_per_dataset['precision_artificial'] +  metrics_per_dataset['precision_real'])/2.\n",
    "metrics_per_dataset['recall'] = (metrics_per_dataset['recall_artificial'] +  metrics_per_dataset['recall_real'])/2.\n",
    "\n",
    "# get optimal parameters (optimizing f1 score)\n",
    "opt_thr, opt_overlap_thr, opt_scaling_factors = metrics_per_dataset.loc[metrics_per_dataset.idxmax()['f1']].name\n",
    "\n",
    "# get optimal scaling factors as list and find corresponding sliding strides\n",
    "opt_scaling_factors = eval(opt_scaling_factors) # transform from str to list\n",
    "opt_sliding_strides = [PARAMS['sliding_strides'][PARAMS['scaling_factors'].index(f)] for f in opt_scaling_factors]\n",
    "\n",
    "\n",
    "tuning_final = {'opt_thr': opt_thr,\n",
    "                'opt_overlap_thr': opt_overlap_thr, \n",
    "                'opt_scaling_factors': opt_scaling_factors, \n",
    "                'opt_sliding_strides': opt_sliding_strides,\n",
    "                }\n",
    "tuning_final.update(metrics_per_dataset.loc[metrics_per_dataset.idxmax()['f1']].to_dict())\n",
    "\n",
    "# save final parameters to json file\n",
    "if logging:\n",
    "    pd.Series(tuning_final).to_json(os.path.join(logdir, 'tuning_final.json'))\n",
    "\n",
    "# show final parameters\n",
    "print(pd.Series(tuning_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some summary figures for the tuning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = plt.subplot(111)\n",
    "sns.scatterplot(data=metrics_df, x='recall', y='precision', hue='scaling_factors')\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "ax.legend(bbox_to_anchor=(1.7,1.0), title='scaling_factors')\n",
    "\n",
    "if logging:\n",
    "    if not os.path.exists(os.path.join(logdir, 'figures')):\n",
    "        os.makedirs(os.path.join(logdir, 'figures'))\n",
    "    fig.savefig(os.path.join(logdir, 'figures/precision_recall.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = plt.subplot(111)\n",
    "sns.lineplot(data=metrics_df, x='overlap_thr', y='f1', hue='scaling_factors')\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "ax.legend(bbox_to_anchor=(1.05,1.0), title='scaling_factors')\n",
    "\n",
    "if logging:\n",
    "    fig.savefig(os.path.join(logdir, 'figures/f1_overlap_thr.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "ax = plt.subplot(111)\n",
    "sns.barplot(data=metrics_df, x='scaling_factors', y='f1')\n",
    "sns.despine()\n",
    "\n",
    "if logging:\n",
    "    fig.savefig(os.path.join(logdir, 'figures/f1_scaling_factors.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_stats_df = pd.DataFrame(tuning_df)\n",
    "image_stats_df.scaling_factors = image_stats_df.scaling_factors.astype(str)\n",
    "\n",
    "image_stats_df = image_stats_df.set_index(['data_type', 'thr', 'overlap_thr', 'scaling_factors'])\n",
    "\n",
    "real_stats = {'TP': [], 'FP': [], 'TN': [], 'FN': []}\n",
    "\n",
    "for sample in image_stats_df.loc[('real', opt_thr, opt_overlap_thr, str(opt_scaling_factors))].iterrows():\n",
    "    real_stats['TP'].extend(sample[1]['TP'])\n",
    "    real_stats['FP'].extend(sample[1]['FP'])\n",
    "    real_stats['TN'].extend(sample[1]['TN'])\n",
    "    real_stats['FN'].extend(sample[1]['FN'])\n",
    "    \n",
    "TP_count = pd.Series(real_stats['TP']).value_counts().reindex(classes, fill_value=0)\n",
    "FP_count = pd.Series(real_stats['FP']).value_counts().reindex(classes, fill_value=0)\n",
    "TN_count = pd.Series(real_stats['TN']).value_counts().reindex(classes, fill_value=0)\n",
    "FN_count = pd.Series(real_stats['FN']).value_counts().reindex(classes, fill_value=0)\n",
    "\n",
    "\n",
    "count_df = pd.concat([TP_count, FP_count, FN_count], axis=1)\n",
    "count_df.columns = ['TP', 'FP', 'FN']\n",
    "\n",
    "sns.set_context('paper')\n",
    "fig = plt.figure(figsize=(25,3))\n",
    "ax = plt.subplot(111)\n",
    "count_df.plot(ax=ax, kind='bar')\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "if logging:\n",
    "    fig.savefig(os.path.join(logdir, 'figures/real_image_stats.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_stats_df = pd.DataFrame(tuning_df)\n",
    "image_stats_df.scaling_factors = image_stats_df.scaling_factors.astype(str)\n",
    "\n",
    "image_stats_df = image_stats_df.set_index(['data_type', 'thr', 'overlap_thr', 'scaling_factors'])\n",
    "\n",
    "real_stats = {'TP': [], 'FP': [], 'TN': [], 'FN': []}\n",
    "\n",
    "for sample in image_stats_df.loc[('artificial', opt_thr, opt_overlap_thr, str(opt_scaling_factors))].iterrows():\n",
    "    real_stats['TP'].extend(sample[1]['TP'])\n",
    "    real_stats['FP'].extend(sample[1]['FP'])\n",
    "    real_stats['TN'].extend(sample[1]['TN'])\n",
    "    real_stats['FN'].extend(sample[1]['FN'])\n",
    "    \n",
    "TP_count = pd.Series(real_stats['TP']).value_counts().reindex(classes, fill_value=0)\n",
    "FP_count = pd.Series(real_stats['FP']).value_counts().reindex(classes, fill_value=0)\n",
    "TN_count = pd.Series(real_stats['TN']).value_counts().reindex(classes, fill_value=0)\n",
    "FN_count = pd.Series(real_stats['FN']).value_counts().reindex(classes, fill_value=0)\n",
    "\n",
    "\n",
    "count_df = pd.concat([TP_count, FP_count, FN_count], axis=1)\n",
    "count_df.columns = ['TP', 'FP', 'FN']\n",
    "\n",
    "sns.set_context('paper')\n",
    "fig = plt.figure(figsize=(25,3))\n",
    "ax = plt.subplot(111)\n",
    "count_df.plot(ax=ax, kind='bar')\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "if logging:\n",
    "    fig.savefig(os.path.join(logdir, 'figures/artificial_image_stats.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate example output pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output_pictures(dataset, savepath, keys):\n",
    "    \"\"\"Small helper function to be applied to both real and artificial dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    for k in keys:\n",
    "        img = dataset[k]['image']\n",
    "        # perform object detection with final model\n",
    "        pred_labels, probabilities, x0, y0, windowsize = model_helpers.object_detection_sliding_window(model, \n",
    "                                                                                         img, \n",
    "                                                                                         preprocess_function, \n",
    "                                                                                         PARAMS['target_size'][0], \n",
    "                                                                                         ind2class, \n",
    "                                                                                         opt_scaling_factors, \n",
    "                                                                                         opt_sliding_strides, \n",
    "                                                                                         opt_thr, \n",
    "                                                                                         opt_overlap_thr)\n",
    "\n",
    "        # visualize results\n",
    "        fig = model_helpers.visualize_predictions(img, \n",
    "                                                  pred_labels, \n",
    "                                                  probabilities, \n",
    "                                                  x0, \n",
    "                                                  y0,\n",
    "                                                  windowsize)\n",
    "\n",
    "\n",
    "        if logging:\n",
    "            if not os.path.exists(savepath):\n",
    "                os.makedirs(savepath)\n",
    "            fig.savefig(os.path.join(savepath, f'real_{k}.png'), bbox_inches='tight')\n",
    "            \n",
    "\n",
    "# save some results for artificial dataset\n",
    "savepath = os.path.join(logdir, 'figures', 'results', 'artificial')\n",
    "generate_output_pictures(val_artificial_data, savepath, keys=np.arange(0, 30, dtype='int'))\n",
    "\n",
    "# save results for real dataset\n",
    "savepath = os.path.join(logdir, 'figures', 'results', 'real')\n",
    "generate_output_pictures(val_real_data, savepath, keys=list(val_real_data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_food",
   "language": "python",
   "name": "deep_food"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
